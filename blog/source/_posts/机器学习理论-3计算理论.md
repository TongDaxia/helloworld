



**计算学习理论**（computational learning theory）

在概率论中，有界的独立随机变量的求和结果与求和数学期望的偏离程度存在一个固定的上界，这一关系可以用 Hoeffding 不等式（Hoeffding's Inequality）来表示。

P[|ν−μ|>ϵ]≤2e^(−2ϵ^2N)

Hoeffding 不等式就变成了对单个模型在训练集上的错误概率和在所有数据上的错误概率之间关系的描述，也就是训练误差和泛化误差的关系。

它说明总会存在一个足够大的样本容量 NN 使两者近似相等，这时就可以根据模型的训练误差来推导其泛化误差，从而获得关于真实情况的一些信息。当训练误差 νν 接近于 0 时，与之接近的泛化误差 μμ 也会接近于 0，据此可以推断出模型在整个的输入空间内都能够以较大的概率逼近真实情况。

**让模型取得较小的泛化误差可以分成两步：一是让训练误差足够小，二是让泛化误差和训练误差足够接近**。

这就是**概率近似正确**”（Probably Approximately Correct, PAC）学习理论

**样本复杂度**（sample complexity）是保证一个概率近似正确解所需要的样本数量。

**VC 维**（Vapnik-Chervonenkis dimension）是对无限假设空间复杂度的一种度量方式，也可以用于给出模型泛化误差在概率意义上的上界。



























